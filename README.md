# ğŸŒ€ Modular Data Ingestion Pipeline using Airflow, S3 & PostgreSQL (Hosted on Render)

## ğŸ“Œ Project Overview

This project demonstrates a **modular, object-oriented data pipeline** built using **Apache Airflow**, **Python**, **AWS S3**, and a **free PostgreSQL database hosted on Render**. It simulates a real-world ETL scenario where raw JSON files are picked from a local directory, moved to cloud storage (S3), processed, transformed, and then loaded into a cloud-hosted database â€” all orchestrated through Airflow.

---

## ğŸš€ Architecture Flow

<img width="2040" height="701" alt="diagram-export-28-07-2025-20_54_21" src="https://github.com/user-attachments/assets/d5956a58-a223-4099-a3ae-f4746fafa993" />

---

## ğŸ› ï¸ Tech Stack
Python (with OOP design)

Apache Airflow

AWS S3 (via boto3)

PostgreSQL (hosted on Render)

configparser for externalized credentials

Logging module for traceability and monitoring

Airflow UI for DAG scheduling and monitoring

---

### âš™ï¸ Features
âœ… Local file availability check

âœ… Upload JSON file to AWS S3

âœ… Read from S3 and flatten JSON using Python

âœ… Create tables dynamically in PostgreSQL (if not exist)

âœ… Transform and load data into Render DB

âœ… Airflow DAG to automate the workflow

âœ… Configurable credentials via config.ini

âœ… Modular, object-oriented codebase

âœ… Comprehensive logging using Python loggers and Airflow task logs

---

### ğŸ”„ Data Flow / Pipeline Stages
Data Availability Check

Checks for new raw JSON files in a local directory.

Upload to S3

If available, the file is uploaded to a configured S3 bucket.

Read & Transform

The JSON is read from S3, flattened into tabular format using pandas.

DB Table Check & Creation

The system checks if the destination table exists in the Render PostgreSQL DB; if not, it creates it.

Data Load

Transformed data is inserted into the target table.

Monitoring & Logging

Each task logs its process using Python loggers and can be monitored through Airflowâ€™s UI.

---

### ğŸ“… Airflow DAG Details
Primary Trigger: Scheduled DAG checks for the presence of a new local JSON file.

Flow:

Task 1: Check local file availability

Task 2: Upload file to S3

Task 3: Read file from S3 & transform

Task 4: Check/Create DB Table

Task 5: Load data into DB

Monitoring: All steps monitored through Airflow's UI & task logs

---

### ğŸ”§ Configuration
All sensitive or environment-specific configurations are stored in a config.ini file, including:

AWS credentials

S3 bucket details

Database connection strings

---

### ğŸ“‚ Project Structure
```text
.
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ data_ingestion_dag.py      # Airflow DAG to orchestrate the data ingestion pipeline
â”‚   â””â”€â”€ debug_check_file.py        # Script for local debugging or DAG validation
â”‚
â”œâ”€â”€ data_uploads/
â”‚   â””â”€â”€ sample_data.json           # Sample raw JSON file used for testing the pipeline
â”‚
â”œâ”€â”€ resources/
â”‚   â””â”€â”€ config_file.ini            # Configuration file with environment variables or connection settings
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ json_ingestion_dag.py  # Object-oriented implementation of the ingestion DAG logic
â”‚   â”‚
â”‚   â”œâ”€â”€ main/
â”‚   â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”‚   â””â”€â”€ load_json_to_rds.py       # Loads transformed JSON data into an RDS table
â”‚   â”‚   â”‚   â””â”€â”€ rds_connector.py          # Establishes connection to RDS using SQLAlchemy or psycopg2
â”‚   â”‚   â”‚   â””â”€â”€ rds_table_manager.py      # Creates or manages RDS tables (DDL operations)
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ s3/
â”‚   â”‚   â”‚   â””â”€â”€ upload_to_s3.py           # Handles uploading raw or transformed data to Amazon S3
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”‚   â””â”€â”€ flatten_json.py           # Utility script to flatten nested JSON structures for easier loading
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ main.py                       # Main orchestrator script (optional CLI entry point or test runner)
â”‚   â”‚
â”‚   â”œâ”€â”€ test/                             # (To be populated) Unit tests for individual components and modules
â”‚
â”œâ”€â”€ README.md                      # Project overview, setup instructions, and pipeline architecture

```

---

### ğŸŒ Deployment Details
Airflow: Locally hosted for DAG orchestration

S3: Used as the intermediate cloud storage

PostgreSQL: Free-tier database hosted on Render

Local Machine: Source for JSON files simulating raw data arrival

---

### ğŸ“ˆ Future Enhancements
Add unit tests and CI/CD integration

Migrate to Dockerized Airflow for cloud deployment

Integrate email alerts or Slack notifications

Add schema validation and error handling layers

---

### ğŸ¤ Let's Connect
This project is part of my portfolio showcasing real-world data engineering skills.

Feel free to explore, clone, or reach out for collaboration or feedback.

ğŸ“§ [amansept22@gmail.com]

ğŸ”— [https://www.linkedin.com/in/amankumarthebiexpert/]

ğŸŒ [https://amankumarbiexpert.my.canva.site/-resume-website]
